{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10677266,"sourceType":"datasetVersion","datasetId":6613999}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-07T12:50:04.675269Z","iopub.execute_input":"2025-02-07T12:50:04.675565Z","iopub.status.idle":"2025-02-07T12:50:05.040372Z","shell.execute_reply.started":"2025-02-07T12:50:04.675539Z","shell.execute_reply":"2025-02-07T12:50:05.039452Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/llm-classification/sample_submission.csv\n/kaggle/input/llm-classification/train.csv\n/kaggle/input/llm-classification/test.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load data\ntrain_data = pd.read_csv(\"/kaggle/input/llm-classification/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/llm-classification/test.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/llm-classification/sample_submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T12:50:05.041435Z","iopub.execute_input":"2025-02-07T12:50:05.041800Z","iopub.status.idle":"2025-02-07T12:50:10.098375Z","shell.execute_reply.started":"2025-02-07T12:50:05.041774Z","shell.execute_reply":"2025-02-07T12:50:10.097625Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\ntrain_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T12:50:10.099694Z","iopub.execute_input":"2025-02-07T12:50:10.100092Z","iopub.status.idle":"2025-02-07T12:50:10.126064Z","shell.execute_reply.started":"2025-02-07T12:50:10.100052Z","shell.execute_reply":"2025-02-07T12:50:10.125403Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"               id             model_a              model_b  \\\n0           30192  gpt-4-1106-preview           gpt-4-0613   \n1           53567           koala-13b           gpt-4-0613   \n2           65089  gpt-3.5-turbo-0613       mistral-medium   \n3           96401    llama-2-13b-chat  mistral-7b-instruct   \n4          198779           koala-13b   gpt-3.5-turbo-0314   \n...           ...                 ...                  ...   \n57472  4294656694          gpt-4-0613             claude-1   \n57473  4294692063          claude-2.0     llama-2-13b-chat   \n57474  4294710549            claude-1           alpaca-13b   \n57475  4294899228              palm-2       tulu-2-dpo-70b   \n57476  4294947231  gemini-pro-dev-api   gpt-4-1106-preview   \n\n                                                  prompt  \\\n0      [\"Is it morally right to try to have a certain...   \n1      [\"What is the difference between marriage lice...   \n2      [\"explain function calling. how would you call...   \n3      [\"How can I create a test set for a very rare ...   \n4      [\"What is the best way to travel from Tel-Aviv...   \n...                                                  ...   \n57472  [\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...   \n57473  [\"In python, implement a naive Bayes with gaus...   \n57474  [\"is it unethical to work on building weapons?...   \n57475  [\"If a bait contains 0,0025% bromadiolon then ...   \n57476  [\"three kids eat three apples in three days, h...   \n\n                                              response_a  \\\n0      [\"The question of whether it is morally right ...   \n1      [\"A marriage license is a legal document that ...   \n2      [\"Function calling is the process of invoking ...   \n3      [\"Creating a test set for a very rare category...   \n4      [\"The best way to travel from Tel Aviv to Jeru...   \n...                                                  ...   \n57472  [\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...   \n57473  [\"Here is an implementation of a naive Bayes c...   \n57474  [\"Working on weapons technology raises some et...   \n57475  [\"Bromadiolone is a rodenticide which is most ...   \n57476                                      [\"27 apples\"]   \n\n                                              response_b  winner_model_a  \\\n0      [\"As an AI, I don't have personal beliefs or o...               1   \n1      [\"A marriage license and a marriage certificat...               0   \n2      [\"Function calling is the process of invoking ...               0   \n3      [\"When building a classifier for a very rare c...               1   \n4      [\"The best way to travel from Tel-Aviv to Jeru...               0   \n...                                                  ...             ...   \n57472  [\"Here is how that mnemonic represents the dig...               1   \n57473  [\"Sure! Here's an implementation of a naive Ba...               1   \n57474  [\"It depends on the context. Weapons can be us...               1   \n57475  [\"As an AI language model, I do not promote or...               0   \n57476  [\"If three kids eat three apples in three days...               1   \n\n       winner_model_b  winner_tie  \n0                   0           0  \n1                   1           0  \n2                   0           1  \n3                   0           0  \n4                   1           0  \n...               ...         ...  \n57472               0           0  \n57473               0           0  \n57474               0           0  \n57475               1           0  \n57476               0           0  \n\n[57477 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>57472</th>\n      <td>4294656694</td>\n      <td>gpt-4-0613</td>\n      <td>claude-1</td>\n      <td>[\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...</td>\n      <td>[\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...</td>\n      <td>[\"Here is how that mnemonic represents the dig...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57473</th>\n      <td>4294692063</td>\n      <td>claude-2.0</td>\n      <td>llama-2-13b-chat</td>\n      <td>[\"In python, implement a naive Bayes with gaus...</td>\n      <td>[\"Here is an implementation of a naive Bayes c...</td>\n      <td>[\"Sure! Here's an implementation of a naive Ba...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57474</th>\n      <td>4294710549</td>\n      <td>claude-1</td>\n      <td>alpaca-13b</td>\n      <td>[\"is it unethical to work on building weapons?...</td>\n      <td>[\"Working on weapons technology raises some et...</td>\n      <td>[\"It depends on the context. Weapons can be us...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57475</th>\n      <td>4294899228</td>\n      <td>palm-2</td>\n      <td>tulu-2-dpo-70b</td>\n      <td>[\"If a bait contains 0,0025% bromadiolon then ...</td>\n      <td>[\"Bromadiolone is a rodenticide which is most ...</td>\n      <td>[\"As an AI language model, I do not promote or...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57476</th>\n      <td>4294947231</td>\n      <td>gemini-pro-dev-api</td>\n      <td>gpt-4-1106-preview</td>\n      <td>[\"three kids eat three apples in three days, h...</td>\n      <td>[\"27 apples\"]</td>\n      <td>[\"If three kids eat three apples in three days...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>57477 rows × 9 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# create \"combined_text\" for combined features\ntrain_data[\"combined_text\"] = train_data[\"prompt\"] + \" \" + train_data[\"response_a\"] + \" \" + train_data[\"response_b\"]\ntest_data[\"combined_text\"] = test_data[\"prompt\"] + \" \" + test_data[\"response_a\"] + \" \" + test_data[\"response_b\"]\n\n# create target context\ntrain_data[\"target\"] = train_data[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].idxmax(axis=1)\nlabel_encoder = LabelEncoder()\n# Encode \"winner_model_a\", \"winner_model_b\", “winner_tie” to int labels\ntrain_data[\"target\"] = label_encoder.fit_transform(train_data[\"target\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T12:50:10.127210Z","iopub.execute_input":"2025-02-07T12:50:10.127436Z","iopub.status.idle":"2025-02-07T12:50:10.471250Z","shell.execute_reply.started":"2025-02-07T12:50:10.127416Z","shell.execute_reply":"2025-02-07T12:50:10.470594Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#Split validation sets from train data\nx_train, x_val, y_train, y_val = train_test_split(\n    train_data[\"combined_text\"], train_data[\"target\"], test_size = 0.2, random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T12:50:10.471999Z","iopub.execute_input":"2025-02-07T12:50:10.472328Z","iopub.status.idle":"2025-02-07T12:50:10.481226Z","shell.execute_reply.started":"2025-02-07T12:50:10.472299Z","shell.execute_reply":"2025-02-07T12:50:10.480534Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Vectorize text using TF-IDF\nvectorizer = TfidfVectorizer(max_features = 5000)\nx_train_tfidf = vectorizer.fit_transform(x_train)\nx_val_tfidf = vectorizer.transform(x_val)\ntest_tfidf = vectorizer.transform(test_data[\"combined_text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T12:50:10.481947Z","iopub.execute_input":"2025-02-07T12:50:10.482265Z","iopub.status.idle":"2025-02-07T12:50:28.165870Z","shell.execute_reply.started":"2025-02-07T12:50:10.482240Z","shell.execute_reply":"2025-02-07T12:50:28.165248Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Define DNN model\nclass DNN(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(DNN, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(64, output_dim),\n            nn.Softmax(dim=1) # 3 classification\n        )\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T13:22:59.578744Z","iopub.execute_input":"2025-02-07T13:22:59.579162Z","iopub.status.idle":"2025-02-07T13:22:59.585068Z","shell.execute_reply.started":"2025-02-07T13:22:59.579131Z","shell.execute_reply":"2025-02-07T13:22:59.584114Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nx_train_scaled = scaler.fit_transform(x_train_tfidf.toarray())\nx_val_scaled = scaler.transform(x_val_tfidf.toarray())\ntest_scaled = scaler.transform(test_tfidf.toarray())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T13:36:54.355473Z","iopub.execute_input":"2025-02-07T13:36:54.355765Z","iopub.status.idle":"2025-02-07T13:37:01.650439Z","shell.execute_reply.started":"2025-02-07T13:36:54.355743Z","shell.execute_reply":"2025-02-07T13:37:01.649764Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# transform data to tensor\nx_train_tensor = torch.tensor(x_train_scaled, dtype = torch.float32)\nx_val_tensor = torch.tensor(x_val_scaled, dtype = torch.float32)\ny_train_tensor = torch.tensor(y_train.to_numpy(), dtype = torch.long)\ny_val_tensor = torch.tensor(y_val.to_numpy(), dtype = torch.long)\n\ntest_tensor = torch.tensor(test_scaled, dtype = torch.float32)\n\ntrain_dataset = TensorDataset(x_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T13:38:23.061791Z","iopub.execute_input":"2025-02-07T13:38:23.062137Z","iopub.status.idle":"2025-02-07T13:38:23.679526Z","shell.execute_reply.started":"2025-02-07T13:38:23.062108Z","shell.execute_reply":"2025-02-07T13:38:23.678562Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Here is the unscalered vision data\n# # transform data to tensor\n# x_train_tensor = torch.tensor(x_train_tfidf.toarray(), dtype = torch.float32)\n# x_val_tensor = torch.tensor(x_val_tfidf.toarray(), dtype = torch.float32)\n# y_train_tensor = torch.tensor(y_train.to_numpy(), dtype = torch.long)\n# y_val_tensor = torch.tensor(y_val.to_numpy(), dtype = torch.long)\n\n# test_tensor = torch.tensor(test_tfidf.toarray(), dtype = torch.float32)\n\n# train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n# train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T12:57:32.619223Z","iopub.execute_input":"2025-02-07T12:57:32.619503Z","iopub.status.idle":"2025-02-07T12:57:36.662101Z","shell.execute_reply.started":"2025-02-07T12:57:32.619483Z","shell.execute_reply":"2025-02-07T12:57:36.661318Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# train DNN\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninput_dim = x_train_tensor.shape[1]\noutput_dim = 3 # 3 classes\nmodel = DNN(input_dim, output_dim).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr = 1e-2)\n\nfor epoch in range(100):\n    model.train()\n    total_loss = 0\n    for x_batch, y_batch in train_loader:\n        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n\n        optimizer.zero_grad()\n        y_pred = model.forward(x_batch)\n        loss = criterion(y_pred, y_batch)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1}: loss: {total_loss:.4f}\")\n\n# validation\nmodel.eval()\nwith torch.no_grad():\n    y_val_pred = model(x_val_tensor.to(device)).argmax(dim=1).cpu().numpy()\n    acc = accuracy_score(y_val_pred, y_val_tensor.numpy())\n    print(f\"DNN validation accuracy:{acc:.4f}\")\n\n    test_probs = model(test_tensor.to(device)).cpu().numpy()\n    submission = sample_submission.copy()\n    submission[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]] = test_probs\n    submission.to_csv(\"submission_DNN.csv\", index = False)\n    print(\"DNN submission saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T13:38:26.983455Z","iopub.execute_input":"2025-02-07T13:38:26.983730Z","iopub.status.idle":"2025-02-07T13:47:57.740412Z","shell.execute_reply.started":"2025-02-07T13:38:26.983706Z","shell.execute_reply":"2025-02-07T13:47:57.739551Z"}},"outputs":[{"name":"stdout","text":"Epoch 1: loss: 1568.5795\nEpoch 2: loss: 1546.6425\nEpoch 3: loss: 1522.9871\nEpoch 4: loss: 1493.9297\nEpoch 5: loss: 1457.5407\nEpoch 6: loss: 1408.3172\nEpoch 7: loss: 1348.8594\nEpoch 8: loss: 1309.2673\nEpoch 9: loss: 1278.1895\nEpoch 10: loss: 1249.8742\nEpoch 11: loss: 1231.8438\nEpoch 12: loss: 1214.8586\nEpoch 13: loss: 1199.1082\nEpoch 14: loss: 1182.3484\nEpoch 15: loss: 1178.1267\nEpoch 16: loss: 1169.8169\nEpoch 17: loss: 1157.9682\nEpoch 18: loss: 1146.8519\nEpoch 19: loss: 1137.1942\nEpoch 20: loss: 1130.5311\nEpoch 21: loss: 1121.6830\nEpoch 22: loss: 1123.5311\nEpoch 23: loss: 1117.3281\nEpoch 24: loss: 1112.0403\nEpoch 25: loss: 1099.2989\nEpoch 26: loss: 1098.0708\nEpoch 27: loss: 1092.9476\nEpoch 28: loss: 1090.3104\nEpoch 29: loss: 1084.7247\nEpoch 30: loss: 1074.9704\nEpoch 31: loss: 1071.6748\nEpoch 32: loss: 1074.4216\nEpoch 33: loss: 1070.9640\nEpoch 34: loss: 1070.5187\nEpoch 35: loss: 1065.0475\nEpoch 36: loss: 1056.3188\nEpoch 37: loss: 1055.9644\nEpoch 38: loss: 1047.4531\nEpoch 39: loss: 1046.6634\nEpoch 40: loss: 1041.5325\nEpoch 41: loss: 1042.7872\nEpoch 42: loss: 1038.8302\nEpoch 43: loss: 1036.6032\nEpoch 44: loss: 1031.5135\nEpoch 45: loss: 1034.2448\nEpoch 46: loss: 1026.5725\nEpoch 47: loss: 1022.6754\nEpoch 48: loss: 1018.7051\nEpoch 49: loss: 1021.0567\nEpoch 50: loss: 1016.1257\nEpoch 51: loss: 1016.6756\nEpoch 52: loss: 1017.2905\nEpoch 53: loss: 1016.1049\nEpoch 54: loss: 1009.8358\nEpoch 55: loss: 1007.9409\nEpoch 56: loss: 1005.4889\nEpoch 57: loss: 1002.7126\nEpoch 58: loss: 997.6540\nEpoch 59: loss: 990.7000\nEpoch 60: loss: 990.7261\nEpoch 61: loss: 989.0960\nEpoch 62: loss: 985.6614\nEpoch 63: loss: 985.7098\nEpoch 64: loss: 989.4683\nEpoch 65: loss: 984.3559\nEpoch 66: loss: 988.0178\nEpoch 67: loss: 990.4619\nEpoch 68: loss: 986.4029\nEpoch 69: loss: 982.4267\nEpoch 70: loss: 977.4013\nEpoch 71: loss: 981.5122\nEpoch 72: loss: 979.9498\nEpoch 73: loss: 976.6604\nEpoch 74: loss: 970.4128\nEpoch 75: loss: 975.8665\nEpoch 76: loss: 971.7501\nEpoch 77: loss: 971.4098\nEpoch 78: loss: 973.0754\nEpoch 79: loss: 974.3684\nEpoch 80: loss: 973.6106\nEpoch 81: loss: 970.6669\nEpoch 82: loss: 971.5428\nEpoch 83: loss: 970.9939\nEpoch 84: loss: 970.5929\nEpoch 85: loss: 972.2249\nEpoch 86: loss: 967.1157\nEpoch 87: loss: 965.9603\nEpoch 88: loss: 969.8536\nEpoch 89: loss: 973.4191\nEpoch 90: loss: 971.5051\nEpoch 91: loss: 969.9740\nEpoch 92: loss: 972.0477\nEpoch 93: loss: 967.7513\nEpoch 94: loss: 968.0551\nEpoch 95: loss: 961.2619\nEpoch 96: loss: 963.9397\nEpoch 97: loss: 966.5288\nEpoch 98: loss: 975.4059\nEpoch 99: loss: 973.4387\nEpoch 100: loss: 979.1712\nDNN validation accuracy:0.3578\nDNN submission saved!\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# # Models to train\n# models = {\n#     \"LogisticRegression\": LogisticRegression(max_iter = 1000, random_state=42),\n#     \"RandomForest\": RandomForestClassifier(n_estimators = 100, random_state=42),\n#     \"SVM\": SVC(probability=True, random_state=42),\n# }\n\n\n\n# # Train each model and save predictions\n# for model_name, model in models.items():\n#     print(f\"Training {model_name} ...\")\n#     model.fit(x_train_tfidf.toarray(), y_train)\n\n#     #validate\n#     y_val_pred = model.predict(x_val_tfidf)\n#     acc = accuracy_score(y_val, y_val_pred)\n#     print(f\"{model_name} validation accuracy: {acc:.4f}\")\n\n#     # predict on test data\n#     test_probs = model.predict_proba(test_tfidf)\n\n#     # create submission file\n#     submission = sample_submission.copy()\n#     submission[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]] = test_probs\n#     submission.to_csv(f\"submission_{model_name}.csv\", index=False)\n#     print(f\"Submission file for {model_name} saved!\")\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-07T12:50:28.173151Z","iopub.execute_input":"2025-02-07T12:50:28.173364Z","iopub.status.idle":"2025-02-07T12:50:28.191207Z","shell.execute_reply.started":"2025-02-07T12:50:28.173345Z","shell.execute_reply":"2025-02-07T12:50:28.190470Z"}},"outputs":[],"execution_count":10}]}